# Bot and Crawler Protection Configuration for Cloud Infrastructure Platform
# Include this file in server blocks to implement bot protection measures

# Define variables to identify bots
map $http_user_agent $is_bot {
    default 0;
    ~*(googlebot|bingbot|yandex|baiduspider|twitterbot|facebookexternalhit|rogerbot|linkedinbot|embedly|quora\ link\ preview|showyoubot|outbrain|pinterest|slackbot|vkShare|W3C_Validator) 1;
}

map $http_user_agent $bad_bot {
    default 0;
    ~*(scrapy|screaming\ frog|semrush|ahrefsbot|majestic|httrack|grapeshot|mj12bot|crawler4j|blexbot|python-requests|spider|scan|nmap) 1;
    ~*(YandexBot|YandexImages|YandexMetrika|YandexAccessibilityBot) 1;
}

map $http_user_agent $is_crawler {
    default 0;
    ~*(bot|crawler|spider) 1;
}

map $http_user_agent $is_empty_ua {
    default 0;
    ~^$ 1;
}

# Block blank user agents
if ($is_empty_ua) {
    return 403;
}

# Rules for known bad bots
if ($bad_bot) {
    return 403;
}

# Rate limiting specifically for crawlers
limit_req_zone $binary_remote_addr$is_crawler zone=crawlers:10m rate=10r/m;

# Block specific request patterns associated with vulnerability scanners
map $request_uri $is_scanner {
    default 0;
    ~*/(wp-admin|wp-login|wp-content|wordpress|wp-includes|phpinfo|phpmyadmin|administrator|joomla|drupal|shell|cgi-bin|htdocs) 1;
    ~*/(.git|.svn|.htaccess|.env|.DS_Store) 1;
    ~*\.(sql|bak|old|backup|zip|tar|gz|config) 1;
}

# Log and block scanning attempts
if ($is_scanner) {
    access_log /var/log/nginx/scanners.log combined;
    return 404;
}

# Allow good bots to bypass rate limiting but verify they're legitimate
map $http_user_agent $valid_googlebot {
    default 0;
    ~*googlebot 1;
}

# Google bot verification function (requires geoip module)
geo $google_validated {
    default 0;
    # Google's IP ranges
    66.249.64.0/19 $valid_googlebot;
    64.233.160.0/19 $valid_googlebot;
}

# Apply special rate limits for crawlers that pass validation
limit_req_zone $binary_remote_addr$google_validated zone=google_bot:10m rate=30r/m;

# Block requests with suspicious query strings
map $args $has_suspicious_args {
    default 0;
    ~*(eval\() 1;
    ~*(javascript:) 1;
    ~*(base64_encode) 1;
    ~*(CONCAT\() 1;
    ~*(<|>|;|'|"|\|) 1;
}

# Block suspicious arguments
if ($has_suspicious_args) {
    return 403;
}

# Usage examples:
#
# location / {
#     if ($bad_bot) {
#         return 403;
#     }
#     
#     # Apply crawler rate limiting
#     limit_req zone=crawlers burst=5 nodelay;
# }
#
# location /api/ {
#     # Block all bots from API
#     if ($is_bot) {
#         return 403;
#     }
# }