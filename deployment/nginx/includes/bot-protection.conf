# Bot and Crawler Protection Configuration for Cloud Infrastructure Platform
# Include this file in server blocks to implement bot protection measures

# Define map blocks outside of server/location context
# Maps for bot detection

# Good bots we want to allow
map $http_user_agent $is_known_bot {
    default 0;
    ~*(googlebot|bingbot|yandex|baiduspider|twitterbot|facebookexternalhit|rogerbot|linkedinbot|embedly|quora\ link\ preview|showyoubot|outbrain|pinterest|slackbot|vkShare|W3C_Validator) 1;
}

# Bad bots we want to block
map $http_user_agent $bad_bot {
    default 0;
    ~*(scrapy|semrush|ahrefsbot|majestic|httrack|grapeshot|mj12bot|blexbot|python-requests|nmap) 1;
    ~*(zgrab|nikto|nessus|sqlmap|acunetix|metasploit|masscan|zmeu|slowloris) 1;
    ~*(FHscan|proxyjudge|proxytester|scrapyproject|YandexMetrika) 1;
}

# Generic crawler detection
map $http_user_agent $is_crawler {
    default 0;
    ~*(bot|crawl|spider|scan|wget|curl) 1;
}

# Empty user agent detection
map $http_user_agent $is_empty_ua {
    default 0;
    ~^$ 1;
}

# Scanner detection based on request URI
map $request_uri $is_scanner {
    default 0;
    ~*/(wp-admin|wp-login|wp-content|wordpress|wp-includes|phpinfo|phpmyadmin|administrator|joomla|drupal|shell|cgi-bin|htdocs) 1;
    ~*/(.git|.svn|.htaccess|.env|.DS_Store|.sql) 1;
    ~*\.(sql|bak|old|backup|zip|tar|gz|config|env|ini) 1;
}

# Define rate limiting zones
limit_req_zone $binary_remote_addr zone=general:10m rate=30r/s;
limit_req_zone $binary_remote_addr$is_crawler zone=crawlers:10m rate=10r/m;

# Rules to be included within server {} or location {} blocks
# These rules must be used within a server or location context

# Bot protection rules - use with "include /etc/nginx/conf.d/bot-protection.conf;"
# Start of context-specific rules

# Block bad bots
if ($bad_bot) {
    return 403;
}

# Block empty user agents
if ($is_empty_ua) {
    return 403;
}

# Log and block scanning attempts
if ($is_scanner) {
    access_log /var/log/nginx/scanners.log combined;
    return 404;
}

# Apply rate limiting to crawlers
if ($is_crawler) {
    limit_req zone=crawlers burst=5 nodelay;
    
    # Add a special header for crawlers we detect
    add_header X-Crawler-Control "rate-limited" always;
    
    # Optional: Log crawler activity separately
    access_log /var/log/nginx/crawlers.log combined;
}

# Apply general rate limiting with a higher burst for regular users
limit_req zone=general burst=20;

# Apply conditional rules based on bot type
if ($is_known_bot = 1) {
    # Allow legitimate bots but log their activity
    access_log /var/log/nginx/known-bots.log combined;
    # Don't apply rate limiting to known good bots
}

# Additional security headers
add_header X-Robots-Tag "noindex, nofollow" always;
add_header X-Content-Type-Options "nosniff" always;

# End of context-specific rules